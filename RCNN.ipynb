{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8129d74c",
   "metadata": {},
   "source": [
    "Turning the ground truth text file into a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b7a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# read lines, split on ';'\n",
    "lines = Path(r\"C:\\Users\\eren\\Desktop\\AI Traffic sign detect\\RCNN\\TrainIJCNN2013\\gt.txt\").read_text().strip().splitlines()\n",
    "records = []\n",
    "for L in lines:\n",
    "    fname, x1, y1, x2, y2, cls = L.split(';')\n",
    "    records.append({\n",
    "        \"filename\": fname,\n",
    "        \"x1\": int(x1),\n",
    "        \"y1\": int(y1),\n",
    "        \"x2\": int(x2),\n",
    "        \"y2\": int(y2),\n",
    "        \"class_id\": int(cls),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(r\"C:\\Users\\eren\\Desktop\\AI Traffic sign detect\\RCNN\\annotations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec8f9d7",
   "metadata": {},
   "source": [
    "GTSDB Dataset & DataLoader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6323600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.ops import box_iou\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "\n",
    "class GTSDBDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transforms=None):\n",
    "        import pandas as pd\n",
    "        self.df = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "        # group annotations by image\n",
    "        self.grouped = self.df.groupby(\"filename\")\n",
    "\n",
    "        # list of unique image names\n",
    "        self.images = list(self.grouped.groups.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        annots = self.grouped.get_group(img_name)\n",
    "\n",
    "        # load image\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # boxes & labels\n",
    "        boxes = torch.tensor(\n",
    "            annots[[\"x1\",\"y1\",\"x2\",\"y2\"]].values,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        labels = torch.tensor(annots[\"class_id\"].values, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "# transforms & loaders\n",
    "import torchvision.transforms as T\n",
    "train_transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomHorizontalFlip(0.5),\n",
    "])\n",
    "val_transforms = T.ToTensor()\n",
    "\n",
    "train_ds = GTSDBDataset(r\"C:\\Users\\eren\\Desktop\\AI Traffic sign detect\\RCNN\\annotations.csv\",r\"C:\\Users\\eren\\Desktop\\AI Traffic sign detect\\RCNN\\TrainIJCNN2013\" , transforms=train_transforms)\n",
    "val_ds   = GTSDBDataset(r\"C:\\Users\\eren\\Desktop\\AI Traffic sign detect\\RCNN\\annotations.csv\",r\"C:\\Users\\eren\\Desktop\\AI Traffic sign detect\\RCNN\\TrainIJCNN2013\", transforms=val_transforms)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6704e3",
   "metadata": {},
   "source": [
    "Prepare Resnet50 for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f706240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=44, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=176, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load COCO‐pretrained model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the head\n",
    "num_classes = 1 + df[\"class_id\"].nunique()  \n",
    "in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe1641",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa76cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 127/127 [1:24:33<00:00, 39.95s/batch, avg_loss=0.3374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 1 done in 5073.2s  Avg Loss: 0.3374\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 127/127 [1:20:04<00:00, 37.83s/batch, avg_loss=0.2846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 2 done in 4804.6s  Avg Loss: 0.2846\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 127/127 [1:26:57<00:00, 41.08s/batch, avg_loss=0.2526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 3 done in 5217.3s  Avg Loss: 0.2526\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 127/127 [1:26:42<00:00, 40.97s/batch, avg_loss=0.2337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 4 done in 5202.9s  Avg Loss: 0.2337\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 127/127 [1:26:49<00:00, 41.02s/batch, avg_loss=0.2290]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 5 done in 5209.6s  Avg Loss: 0.2290\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 127/127 [1:20:58<00:00, 38.26s/batch, avg_loss=0.2246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 6 done in 4858.6s  Avg Loss: 0.2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 127/127 [1:21:05<00:00, 38.31s/batch, avg_loss=0.2223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 7 done in 4865.6s  Avg Loss: 0.2223\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 127/127 [1:22:02<00:00, 38.76s/batch, avg_loss=0.2211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 8 done in 4922.5s  Avg Loss: 0.2211\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 127/127 [1:21:42<00:00, 38.60s/batch, avg_loss=0.2203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 9 done in 4902.1s  Avg Loss: 0.2203\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 127/127 [1:20:43<00:00, 38.14s/batch, avg_loss=0.2244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 10 done in 4843.7s  Avg Loss: 0.2244\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    start_epoch = time.time()\n",
    "\n",
    "    # wrap loader in tqdm, but we'll use enumerate() to get batch_i\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    for batch_i, (imgs, targets) in enumerate(pbar, start=1):\n",
    "        # move data\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        tgt  = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # forward + loss\n",
    "        loss_dict = model(imgs, tgt)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # backward + step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate\n",
    "        running_loss += loss.item()\n",
    "        avg_loss = running_loss / batch_i  # use batch_i, not pbar.n\n",
    "\n",
    "        # update bar with our avg_loss\n",
    "        pbar.set_postfix(avg_loss=f\"{avg_loss:.4f}\")\n",
    "\n",
    "    # scheduler step & epoch summary\n",
    "    lr_scheduler.step()\n",
    "    epoch_time = time.time() - start_epoch\n",
    "    print(f\"→ Epoch {epoch+1} done in {epoch_time:.1f}s  Avg Loss: {avg_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ec920",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee154220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Weights saved to fasterrcnn_gtsdb_weights.pth\n"
     ]
    }
   ],
   "source": [
    "# Save only the model’s parameters\n",
    "torch.save(model.state_dict(), \"fasterrcnn_gtsdb_weights.pth\")\n",
    "print(\"→ Weights saved to fasterrcnn_gtsdb_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55505641",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b68a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Model loaded, ready for inference\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch\n",
    "\n",
    "# 1. Rebuild the exact same architecture\n",
    "num_classes = 1 + 43   # change if you have a different number of classes\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, num_classes)\n",
    "\n",
    "# 2. Load your saved weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"fasterrcnn_gtsdb_weights.pth\", map_location=device))\n",
    "model.to(device).eval()\n",
    "print(\"→ Model loaded, ready for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167b202f",
   "metadata": {},
   "source": [
    "Function to evaluate overall precision & recall @ (score_threshold, iou_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pr_iou(model, data_loader, device,\n",
    "                    iou_threshold=0.5, score_threshold=0.05):\n",
    "\n",
    "    model.eval()\n",
    "    tp = fp = fn = 0\n",
    "    iou_accum = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in data_loader:\n",
    "            imgs = [img.to(device) for img in imgs]\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            for out, tgt in zip(outputs, targets):\n",
    "                gt_boxes = tgt[\"boxes\"].to(device)\n",
    "                # filter predictions by score\n",
    "                keep = out[\"scores\"] > score_threshold\n",
    "                pred_boxes = out[\"boxes\"][keep]\n",
    "\n",
    "                if len(pred_boxes) == 0:\n",
    "                    fn += len(gt_boxes)\n",
    "                    continue\n",
    "                if len(gt_boxes) == 0:\n",
    "                    fp += len(pred_boxes)\n",
    "                    continue\n",
    "\n",
    "                # IoU matrix\n",
    "                ious = box_iou(pred_boxes, gt_boxes)\n",
    "\n",
    "                # True positives: preds matching any gt\n",
    "                matched_pred = (ious >= iou_threshold).any(dim=1)\n",
    "                tp += matched_pred.sum().item()\n",
    "                fp += (~matched_pred).sum().item()\n",
    "\n",
    "                # False negatives: gt not matched by any pred\n",
    "                matched_gt = (ious >= iou_threshold).any(dim=0)\n",
    "                fn += (~matched_gt).sum().item()\n",
    "\n",
    "                # record IoUs of matched preds for mean‑IoU\n",
    "                # for each pred, take its best IoU\n",
    "                best_ious, _ = ious.max(dim=1)\n",
    "                iou_accum.extend(best_ious[matched_pred].cpu().tolist())\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    mean_iou  = sum(iou_accum) / len(iou_accum) if iou_accum else 0.0\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"mean_iou\": mean_iou\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d1b24",
   "metadata": {},
   "source": [
    "Use torchmetrics to compute mAP@0.5 for all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_map(model, data_loader, device):\n",
    "\n",
    "    metric = MeanAveragePrecision(\n",
    "        iou_type='bbox',\n",
    "        iou_thresholds=[0.5],\n",
    "        class_metrics=True\n",
    "    ).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in data_loader:\n",
    "            imgs = [img.to(device) for img in imgs]\n",
    "            targs = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            preds = model(imgs)\n",
    "            metric.update(preds, targs)\n",
    "\n",
    "    return metric.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59092a88",
   "metadata": {},
   "source": [
    "Run Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af7cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Running quick PR/IoU evaluation on validation set…\n",
      "Precision@0.05/IoU0.5: 0.9237\n",
      "Recall   @0.05/IoU0.5: 0.9981\n",
      "Mean IoU          : 0.8339\n",
      "\n",
      "→ Running mAP@0.5 evaluation via torchmetrics…\n",
      "mAP@0.5: 0.1841\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmAP@0.5: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmap_results[\u001b[33m'\u001b[39m\u001b[33mmap_50\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# if you want per‑class breakdown, assuming you have a class_names dict:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, cls_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclass_names\u001b[49m.items():\n\u001b[32m     16\u001b[39m     ap   = map_results[\u001b[33m'\u001b[39m\u001b[33map_per_class\u001b[39m\u001b[33m'\u001b[39m][idx].item()\n\u001b[32m     17\u001b[39m     prec = map_results[\u001b[33m'\u001b[39m\u001b[33mprecision_per_class\u001b[39m\u001b[33m'\u001b[39m][idx].item()\n",
      "\u001b[31mNameError\u001b[39m: name 'class_names' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"→ Running quick PR/IoU evaluation on validation set…\")\n",
    "pr_iou = evaluate_pr_iou(model, val_loader, device,\n",
    "                         iou_threshold=0.5, score_threshold=0.05)\n",
    "print(f\"Precision@0.05/IoU0.5: {pr_iou['precision']:.4f}\")\n",
    "print(f\"Recall   @0.05/IoU0.5: {pr_iou['recall']:.4f}\")\n",
    "print(f\"Mean IoU          : {pr_iou['mean_iou']:.4f}\\n\")\n",
    "\n",
    "print(\"→ Running mAP@0.5 evaluation via torchmetrics…\")\n",
    "map_results = evaluate_map(model, val_loader, device)\n",
    "print(f\"mAP@0.5: {map_results['map_50']:.4f}\\n\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
